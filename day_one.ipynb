{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6e41f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U -q \"google-genai==1.7.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1deeea21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.11\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1428091",
   "metadata": {},
   "source": [
    "Import SDK and additional helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb290d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types # type: ignore\n",
    "\n",
    "from IPython.display import HTML, Markdown, display # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc76a8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY = \"AIzaSyCpjnav1edbctCCo2_x5rz2emw0keu6djk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5f32250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, imagine you have a box of crayons, and each crayon is labeled with a word like \"happy,\" \"sad,\" \"angry,\" \"excited,\" \"cat,\" \"dog,\" \"tree,\" \"sun.\"\n",
       "\n",
       "Now, you want to put these crayons into a special arrangement so that crayons that are similar go together.\n",
       "\n",
       "For example, \"happy\" and \"excited\" are similar feelings, so you'd put them close together.  \"Cat\" and \"dog\" are both animals, so they'd also go near each other.  \"Tree\" and \"sun\" both belong outside so you could put them near each other.\n",
       "\n",
       "**Embeddings are like a secret map for those crayons!**\n",
       "\n",
       "Instead of physically arranging the crayons, imagine you write down a special code number for each crayon.  This code tells you where the crayon \"lives\" on the map.\n",
       "\n",
       "*   \"Happy\" might get a code like `[0.8, 0.2]`\n",
       "*   \"Excited\" might get a code like `[0.75, 0.25]` (close to \"happy\"!)\n",
       "*   \"Sad\" might get a code like `[0.2, 0.8]` (far away from \"happy\")\n",
       "*   \"Cat\" might get a code like `[0.6, 0.4]`\n",
       "*   \"Dog\" might get a code like `[0.5, 0.3]` (close to \"cat\")\n",
       "\n",
       "The numbers in the code tell you something about that word.  Maybe the first number is how \"positive\" the word is, and the second number is how \"negative\" it is.\n",
       "\n",
       "**Why is this useful?**\n",
       "\n",
       "Well, if you give these codes to a computer, it can understand which words are similar! The computer can see that the codes for \"happy\" and \"excited\" are close together, so it knows those words are related.\n",
       "\n",
       "So, embeddings are like secret codes that help computers understand the meaning of words and how they relate to each other!  It's like giving the computer a way to \"feel\" the same way about words that we do.\n",
       "\n",
       "**In short:**\n",
       "\n",
       "*   **Words are like crayons.**\n",
       "*   **Embeddings are like secret codes that tell you where each crayon lives on a map.**\n",
       "*   **The map shows which crayons (words) are similar to each other.**\n",
       "*   **Computers use these codes to understand words and their meanings.**\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=\"Explain embeddings to me like I'm a kid.\")\n",
    "\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3e06d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, imagine you have a really, really smart puppy, but it's made of computer code! That's kind of what AI is.\n",
       "\n",
       "**Instead of fur and a wagging tail, this puppy has:**\n",
       "\n",
       "*   **A brain made of numbers and instructions:** These instructions tell it what to do.\n",
       "*   **The ability to learn:** You can show it lots of pictures of cats and dogs, and it will learn the difference between them!\n",
       "*   **The power to make guesses:** After seeing enough pictures, it can guess whether a new picture is a cat or a dog, even if it's never seen that exact picture before.\n",
       "\n",
       "**So, AI is like a really smart machine that can:**\n",
       "\n",
       "*   **Learn things:** Just like you learn things at school.\n",
       "*   **Solve problems:** Like figuring out a puzzle.\n",
       "*   **Make decisions:** Like choosing the best route to get to the park.\n",
       "\n",
       "**You see AI all around you!**\n",
       "\n",
       "*   When you ask Siri or Alexa a question, that's AI!\n",
       "*   When you play a video game and the bad guys get smarter the more you play, that's AI!\n",
       "*   When your parents use Google Maps to find the fastest way to grandma's house, that's AI!\n",
       "\n",
       "**It's like having a super-smart friend who can help you with lots of things, but this friend is made of computer code!**\n",
       "\n",
       "**Important Note:** AI can be super helpful, but it's important for grown-ups to make sure it's used in a good way and that it doesn't make mistakes that could hurt people. Just like your puppy needs training, AI needs to be trained carefully!\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=\"Explain AI to me like I'm a kid.\")\n",
    "\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b10afe1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hello Ayodeji! It's nice to meet you. How can I help you today?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = client.chats.create(model='gemini-2.0-flash', history=[])\n",
    "response = chat.send_message('Hello! My name is Ayodeji.')\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1d6f1e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, here's something interesting about Arsenal that you might not know:\n",
       "\n",
       "**Arsenal was originally founded by workers at the Royal Arsenal in Woolwich, South London in 1886. They were called Dial Square F.C., after a workshop inside the Arsenal.**\n",
       "\n",
       "What's fascinating is that the club's roots are firmly planted in industrial labor and the defense industry. This working-class origin is a stark contrast to the often glamorous and commercially-driven nature of modern football, and it's a part of Arsenal's identity that many fans still connect with.\n",
       "\n",
       "Did you know that, or is that new information for you?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat.send_message('Can you tell me something interesting about Arsenal?')\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "643fd319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "That's fantastic! It's great to hear from a fellow Gooner (assuming you are one!). What is it that you love most about Arsenal? Is it a particular player, era, style of play, or something else entirely? I'd love to hear more about your Arsenal passion!\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat.send_message('I love Arsenal FC?')\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6dc06edd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Ah, the beautiful game! Arsenal is definitely known for its attractive, attacking style. What aspects of their \"brand of football\" appeal to you the most? Is it the:\n",
       "\n",
       "*   **Fluid passing and movement?** (Think Wenger-era)\n",
       "*   **Emphasis on youth development?**\n",
       "*   **Commitment to entertaining play?**\n",
       "*   **Something else entirely?**\n",
       "\n",
       "I'm curious to know what specific elements stand out to you!\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat.send_message('The brand of football')\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99d81bd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Yes, Ayodeji, I remember your name is Ayodeji. I am designed to remember details from our conversation.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat.send_message('Do you remember what my name is?')\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bacd8446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Yes, Ayodeji. You said you love Arsenal FC, specifically \"the brand of football\" they play. You also elaborated that you liked the style of play.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat.send_message('Do you remember what I said I love?')\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3802506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/chat-bison-001\n",
      "models/text-bison-001\n",
      "models/embedding-gecko-001\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-002\n",
      "models/gemini-1.5-flash-8b\n",
      "models/gemini-1.5-flash-8b-001\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "models/gemini-1.5-flash-8b-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0924\n",
      "models/gemini-2.5-pro-exp-03-25\n",
      "models/gemini-2.5-pro-preview-03-25\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-exp-image-generation\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-2.0-pro-exp\n",
      "models/gemini-2.0-pro-exp-02-05\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.0-flash-thinking-exp-01-21\n",
      "models/gemini-2.0-flash-thinking-exp\n",
      "models/gemini-2.0-flash-thinking-exp-1219\n",
      "models/learnlm-1.5-pro-experimental\n",
      "models/gemma-3-1b-it\n",
      "models/gemma-3-4b-it\n",
      "models/gemma-3-12b-it\n",
      "models/gemma-3-27b-it\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/gemini-embedding-exp-03-07\n",
      "models/gemini-embedding-exp\n",
      "models/aqa\n",
      "models/imagen-3.0-generate-002\n"
     ]
    }
   ],
   "source": [
    "for model in client.models.list():\n",
    "  print(model.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99ecaf12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'description': 'Gemini 2.0 Flash',\n",
      " 'display_name': 'Gemini 2.0 Flash',\n",
      " 'input_token_limit': 1048576,\n",
      " 'name': 'models/gemini-2.0-flash',\n",
      " 'output_token_limit': 8192,\n",
      " 'supported_actions': ['generateContent', 'countTokens'],\n",
      " 'tuned_model_info': {},\n",
      " 'version': '2.0'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for model in client.models.list():\n",
    "  if model.name == 'models/gemini-2.0-flash':\n",
    "    pprint(model.to_json_dict())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922e7877",
   "metadata": {},
   "source": [
    "Explore generation parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a890ad",
   "metadata": {},
   "source": [
    "Output Length - determines the number of tokens in the output. Does not care about completeness, for completeness - try prompt engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2c7c8994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## The Humble Olive: A Cornerstone of Modern Society\n",
       "\n",
       "The olive, a small, unassuming fruit, often relegated to the role of a garnish or a quirky addition to a cocktail, belies its profound importance in modern society. More than just a culinary staple, the olive and its derived products, particularly olive oil, have woven themselves into the fabric of our cultures, influencing our economies, health, and even our environmental consciousness. From the ancient Mediterranean basin to the burgeoning global market, the olive's impact continues to resonate, proving its enduring significance in the 21st century.\n",
       "\n",
       "One of the most prominent contributions of the olive lies in its nutritional value and its derivative, olive oil, a cornerstone of the celebrated Mediterranean diet. This dietary pattern, long recognized for its positive impact on health and longevity, emphasizes the consumption of fruits, vegetables, whole grains, and, crucially, olive oil. Olive oil, particularly extra virgin olive oil (EVOO), is rich in monounsaturated fatty acids"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_config = types.GenerateContentConfig(max_output_tokens=200)\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=short_config,\n",
    "    contents='Write a 1000 word essay on the importance of olives in modern society.')\n",
    "\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "20dc6400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Organizations can leverage the Gemini API in numerous ways to enhance their products, services, and internal operations. Here's a breakdown of how:\n",
       "\n",
       "**I. Core Use Cases and Applications:**\n",
       "\n",
       "*   **Content Creation & Generation:**\n",
       "    *   **Marketing Copy:** Generate ad copy, social media posts, email subject lines, and landing page content.\n",
       "    *   **Article and Blog Writing:** Automate the creation of articles, blog posts, summaries, and SEO-optimized content.\n",
       "    *   **Product Descriptions:** Create compelling and accurate product descriptions for e-commerce platforms.\n",
       "    *   **Scriptwriting:** Assist with screenplay, dialogue, and narrative generation.\n",
       "    *   **Code Generation:** (depending on the API's capabilities) Generate code snippets, boilerplate, or even full programs based on natural language descriptions.\n",
       "    *   **Presentation Creation:** Quickly draft outlines, slide content, and speaker notes for presentations.\n",
       "\n",
       "*   **Customer Service & Support"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=short_config,\n",
    "    contents='How can organisations leverage the Gemini API?')\n",
    "\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f6c22a",
   "metadata": {},
   "source": [
    "Temperature - controls the degree of randomness in output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a0321615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Magenta\n",
      " -------------------------\n",
      "Turquoise\n",
      " -------------------------\n",
      "Azure\n",
      " -------------------------\n",
      "Turquoise\n",
      " -------------------------\n",
      "Cerulean\n",
      " -------------------------\n"
     ]
    }
   ],
   "source": [
    "high_temp_config = types.GenerateContentConfig(temperature=2.0)\n",
    "\n",
    "for _ in range(5):\n",
    "  response = client.models.generate_content(\n",
    "      model='gemini-2.0-flash',\n",
    "      config=high_temp_config,\n",
    "      contents='Pick a random colour... (respond in a single word)')\n",
    "\n",
    "  if response.text:\n",
    "    print(response.text, '-' * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ab5e5062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure\n",
      " -------------------------\n",
      "Azure\n",
      " -------------------------\n",
      "Azure\n",
      " -------------------------\n",
      "Azure\n",
      " -------------------------\n",
      "Azure\n",
      " -------------------------\n"
     ]
    }
   ],
   "source": [
    "low_temp_config = types.GenerateContentConfig(temperature=0.0)\n",
    "\n",
    "for _ in range(5):\n",
    "  response = client.models.generate_content(\n",
    "      model='gemini-2.0-flash',\n",
    "      config=low_temp_config,\n",
    "      contents='Pick a random colour... (respond in a single word)')\n",
    "\n",
    "  if response.text:\n",
    "    print(response.text, '-' * 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022ff840",
   "metadata": {},
   "source": [
    "Top-P - controls the diversity in model's output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "608e5f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Jasper wasn't your average tabby. Sure, he enjoyed sunbeams and the occasional feathered toy, but a restless spirit thrummed beneath his striped fur. He yearned for more than the predictable rhythm of naps and tuna. He craved adventure.\n",
       "\n",
       "One crisp autumn morning, adventure presented itself in the form of an open window. The scent of damp earth and decaying leaves, a symphony of the wild, wafted in, beckoning him. He hesitated only a moment, a flicker of guilt for abandoning his human, Mrs. Higgins, then leaped onto the sill and into the unknown.\n",
       "\n",
       "The world exploded with sensory overload. Towering trees scraped the sky, their leaves a kaleidoscope of reds and golds. Squirrels chattered insults from branches, and the air buzzed with the frantic energy of preparing for winter. Jasper, emboldened by his newfound freedom, stalked through the undergrowth, a miniature tiger in a leafy jungle.\n",
       "\n",
       "He encountered a grumpy hedgehog, who huffed and puffed at him, and a family of field mice, who scattered at his approach. He even dared to cross a babbling brook, teetering precariously on moss-covered stones, his tail twitching with concentration.\n",
       "\n",
       "The sun began to dip below the horizon, painting the sky in hues of orange and purple. A chill wind whispered through the trees, reminding Jasper of the warmth of Mrs. Higgins' lap. A pang of loneliness, sharp and unexpected, pierced his adventurous spirit.\n",
       "\n",
       "He turned back, retracing his steps, the forest now a shadowy labyrinth. Fear gnawed at him, the rustling leaves sounding like menacing whispers. He quickened his pace, his paws padding softly on the damp earth.\n",
       "\n",
       "Finally, he saw it – the familiar glow of his house, a beacon in the encroaching darkness. He scrambled up the ivy-covered wall, his claws finding purchase in the rough brick. He reached the open window, his heart pounding with relief.\n",
       "\n",
       "He slipped inside, landing silently on the plush carpet. Mrs. Higgins was in her armchair, knitting, her brow furrowed with worry. When she saw him, her face lit up.\n",
       "\n",
       "\"Jasper! Where have you been?\" she cried, scooping him into her arms. He purred, burying his face in her soft cardigan, the scent of lavender and home filling his senses.\n",
       "\n",
       "He had tasted adventure, and it was exhilarating. But as he curled up on Mrs. Higgins' lap, the warmth of the fire radiating through him, he realized that the greatest adventure of all was coming home. He was, after all, just a cat. And sometimes, a cat just needs his human.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config = types.GenerateContentConfig(\n",
    "    # These are the default values for gemini-2.0-flash.\n",
    "    temperature=2.0,\n",
    "    top_p=0.0,\n",
    ")\n",
    "\n",
    "story_prompt = \"You are a creative writer. Write a short story in not more than 500 words about a cat who goes on an adventure.\"\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=model_config,\n",
    "    contents=story_prompt)\n",
    "\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ded8d738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, let's break down the difference between the `temperature` and `top_p` attributes in the Gemini API, and how they influence the generated text:\n",
       "\n",
       "**Temperature:**\n",
       "\n",
       "*   **What it does:** Temperature controls the randomness of the output. It affects the probability distribution of the next token (word) that the model generates.\n",
       "*   **Range:** Typically between 0.0 and 1.0 (though some models might allow slightly higher values).\n",
       "*   **How it works:**\n",
       "    *   **Lower temperature (closer to 0.0):** The model becomes more deterministic and predictable. It will tend to choose the most likely next token based on the training data and the input prompt.  This results in more conservative, focused, and factual outputs.  Good for tasks where precision and accuracy are paramount.\n",
       "    *   **Higher temperature (closer to 1.0):** The model becomes more adventurous and creative. It will consider less likely tokens, increasing the chances of surprising, novel, and even slightly \"off the wall\" responses. Good for tasks like brainstorming, creative writing, or generating diverse ideas.\n",
       "*   **Analogy:** Think of it like adjusting the focus of a camera.  Low temperature is like a sharp focus, concentrating on the most obvious choice. High temperature is like a soft focus, allowing more possibilities into view.\n",
       "*   **Key Use Cases:**\n",
       "    *   **0.0 - 0.4:** Fact retrieval, code generation, tasks requiring accuracy.\n",
       "    *   **0.5 - 0.7:** Standard, balanced responses.\n",
       "    *   **0.8 - 1.0:** Creative writing, brainstorming, generating diverse ideas.\n",
       "\n",
       "**Top_P (Nucleus Sampling):**\n",
       "\n",
       "*   **What it does:** Top_P (or \"nucleus sampling\") focuses on a set of the *most probable* tokens and redistributes the probability mass among them. It's a dynamic selection process.\n",
       "*   **Range:** Typically between 0.0 and 1.0.\n",
       "*   **How it works:**\n",
       "    1.  The model calculates the probability of each possible next token.\n",
       "    2.  It sorts the tokens by their probability in descending order.\n",
       "    3.  It starts accumulating the probabilities of the top tokens until the cumulative probability reaches the `top_p` value.\n",
       "    4.  Only the tokens in this \"nucleus\" are considered for the next token selection.\n",
       "    5.  The probabilities within the nucleus are renormalized (scaled so they sum to 1).\n",
       "*   **Analogy:** Imagine a race.  Top_P is like saying, \"Let's only bet on the top X runners who have a combined Y% chance of winning.\" The size of the group \"X\" changes dynamically based on the probabilities of the runners.\n",
       "*   **Key Use Cases:**\n",
       "    *   **0.1-0.4:** Highly coherent and focused outputs, may lack some creativity.\n",
       "    *   **0.5-0.9:** Good balance between coherence and creativity.\n",
       "    *   **1.0:** All tokens are considered (equivalent to turning off Top_P).\n",
       "\n",
       "**Key Differences & How They Interact:**\n",
       "\n",
       "*   **Fixed vs. Dynamic:** Temperature is a global scaling factor applied to all probabilities. Top_P dynamically selects a subset of tokens based on their probabilities.\n",
       "*   **Control Level:** Temperature provides a more general control over randomness. Top_P provides a more targeted control by focusing on the most likely options.\n",
       "*   **Combination:** You can use both `temperature` and `top_p` together. Typically, it's recommended to use *either* `temperature` *or* `top_p`, but not both.  Using both can sometimes lead to unpredictable results as they are both influencing the probability distribution.\n",
       "*   **When to Choose:**\n",
       "    *   **Use Temperature:** When you want a simple way to control the overall randomness of the output.\n",
       "    *   **Use Top_P:** When you want more control over the coherence and diversity of the output, and want to ensure that the generated text stays within a reasonable range of possibilities. Top_P tends to avoid outputs that are completely nonsensical or irrelevant.\n",
       "\n",
       "**In summary:**\n",
       "\n",
       "*   **Temperature:**  Broadly controls randomness, affecting the likelihood of all possible tokens. Higher = more random.\n",
       "*   **Top_P:** Selects a dynamic subset of the most probable tokens, renormalizes their probabilities, and samples from that subset.  Helps maintain coherence while still allowing for some creativity.\n",
       "\n",
       "**Recommendation:**  Experiment to see which works best for your specific use case! If you're unsure, start with a moderate `top_p` value (e.g., 0.7) and adjust as needed.  If you are using `temperature` start with a moderate value like 0.7 as well.  It's generally recommended to only use one or the other at a time.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat.send_message(\"What is the difference between the temperate and top-p attribute in the Gemini API?\")\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f151fd19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Yes, that's a correct and concise way to put it, Ayodeji!\n",
       "\n",
       "*   **High `top_p` (close to 1.0):** Effectively covers all tokens.  Since the cumulative probability reaches 1.0 quickly, all possible tokens are included in the \"nucleus\" and can be selected.  This is similar to not using `top_p` at all.\n",
       "*   **Low `top_p`:** Selects *only* the tokens with the highest probabilities until their *cumulative* probability reaches the `top_p` value. Tokens below that threshold are excluded from the sampling process.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat.send_message(\"So a high top-p covers all tokens, but a low top-p only selects tokens that are above the probability threshold?\")\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c612246d",
   "metadata": {},
   "source": [
    "Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2bb3d9",
   "metadata": {},
   "source": [
    "Zero-shot Prompting - direct description of request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "047df0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEUTRAL\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_config = types.GenerateContentConfig(\n",
    "    temperature=0.1,\n",
    "    top_p=1,\n",
    "    max_output_tokens=5,\n",
    ")\n",
    "\n",
    "zero_shot_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE, if there are ratings, \n",
    "ratings between 5 and 6.5 should be considered neutral, above should be considered positive\n",
    "Review: \"Her\" is a disturbing study revealing the direction\n",
    "humanity is headed if AI is allowed to keep evolving,\n",
    "unchecked. I would rate it 5/10.\n",
    "Sentiment: \"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=model_config,\n",
    "    contents=zero_shot_prompt)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e516be",
   "metadata": {},
   "source": [
    "ENUM - Constrain output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "20c96582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral\n"
     ]
    }
   ],
   "source": [
    "import enum\n",
    "\n",
    "class Sentiment(enum.Enum):\n",
    "    POSITIVE = \"positive\"\n",
    "    NEUTRAL = \"neutral\"\n",
    "    NEGATIVE = \"negative\"\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=types.GenerateContentConfig(\n",
    "        response_mime_type=\"text/x.enum\",\n",
    "        response_schema=Sentiment\n",
    "    ),\n",
    "    contents=zero_shot_prompt)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "05cdab7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment.NEUTRAL\n",
      "<enum 'Sentiment'>\n"
     ]
    }
   ],
   "source": [
    "enum_response = response.parsed\n",
    "print(enum_response)\n",
    "print(type(enum_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71288aa4",
   "metadata": {},
   "source": [
    "One-shot - One xample of expected response is provided.\n",
    "Few-shot - Multiple examples are provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "784a51da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\"size\": \"large\",\n",
      "\"type\": \"normal\",\n",
      "\"ingredients\": [\"cheese\", \"pineapple\"]\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "few_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n",
    "\n",
    "EXAMPLE:\n",
    "I want a small pizza with cheese, tomato sauce, and pepperoni.\n",
    "JSON Response:\n",
    "```\n",
    "{\n",
    "\"size\": \"small\",\n",
    "\"type\": \"normal\",\n",
    "\"ingredients\": [\"cheese\", \"tomato sauce\", \"pepperoni\"]\n",
    "}\n",
    "```\n",
    "\n",
    "EXAMPLE:\n",
    "Can I get a large pizza with tomato sauce, basil and mozzarella\n",
    "JSON Response:\n",
    "```\n",
    "{\n",
    "\"size\": \"large\",\n",
    "\"type\": \"normal\",\n",
    "\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n",
    "}\n",
    "```\n",
    "\n",
    "ORDER:\n",
    "\"\"\"\n",
    "\n",
    "customer_order = \"Give me a large with cheese & pineapple\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=types.GenerateContentConfig(\n",
    "        temperature=0.1,\n",
    "        top_p=1,\n",
    "        max_output_tokens=250,\n",
    "    ),\n",
    "    contents=[few_shot_prompt, customer_order])\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43515b58",
   "metadata": {},
   "source": [
    "Parse output in Json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3981f9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"size\": \"large\",\n",
      "  \"ingredients\": [\"apple\", \"chocolate\"],\n",
      "  \"type\": \"dessert\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import typing_extensions as typing # type: ignore\n",
    "\n",
    "class PizzaOrder(typing.TypedDict):\n",
    "    size: str\n",
    "    ingredients: list[str]\n",
    "    type: str\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=types.GenerateContentConfig(\n",
    "        temperature=0.1,\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=PizzaOrder,\n",
    "    ),\n",
    "    contents=\"Can I have a large dessert pizza with apple and chocolate\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2590e2d",
   "metadata": {},
   "source": [
    "Chain of Thought - to address hallucinations (factual or reasoning incorrectness)\n",
    "Model outputs intermediate reasoning steps, costs more to run, but achieves better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b4e3bfe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "52\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\n",
    "am 20 years old. How old is my partner? Return the answer directly.\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    contents=prompt)\n",
    "\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ec9906b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's how to solve the problem:\n",
       "\n",
       "1.  When you were 4, your partner was 3 * 4 = 12 years old.\n",
       "2.  The age difference between you and your partner is 12 - 4 = 8 years.\n",
       "3.  Since you are now 20, your partner is 20 + 8 = 28 years old.\n",
       "\n",
       "28\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "additional_prompt = \"Think step by step\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    contents=[prompt, additional_prompt])\n",
    "\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d76f3a",
   "metadata": {},
   "source": [
    "Reason and Act - ReAct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6431e49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_instructions = \"\"\"\n",
    "Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation,\n",
    "Observation is understanding relevant information from an Action's output and Action can be one of three types:\n",
    " (1) <search>entity</search>, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\n",
    "     will return some similar entities to search and you can try to search the information from those topics.\n",
    " (2) <lookup>keyword</lookup>, which returns the next sentence containing keyword in the current context. This only does exact matches,\n",
    "     so keep your searches short.\n",
    " (3) <finish>answer</finish>, which returns the answer and finishes the task.\n",
    "\"\"\"\n",
    "\n",
    "example1 = \"\"\"Question\n",
    "Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n",
    "\n",
    "Thought 1\n",
    "The question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n",
    "\n",
    "Action 1\n",
    "<search>Milhouse</search>\n",
    "\n",
    "Observation 1\n",
    "Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n",
    "\n",
    "Thought 2\n",
    "The paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n",
    "\n",
    "Action 2\n",
    "<lookup>named after</lookup>\n",
    "\n",
    "Observation 2\n",
    "Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n",
    "\n",
    "Thought 3\n",
    "Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n",
    "\n",
    "Action 3\n",
    "<finish>Richard Nixon</finish>\n",
    "\"\"\"\n",
    "\n",
    "example2 = \"\"\"Question\n",
    "What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n",
    "\n",
    "Thought 1\n",
    "I need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n",
    "\n",
    "Action 1\n",
    "<search>Colorado orogeny</search>\n",
    "\n",
    "Observation 1\n",
    "The Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n",
    "\n",
    "Thought 2\n",
    "It does not mention the eastern sector. So I need to look up eastern sector.\n",
    "\n",
    "Action 2\n",
    "<lookup>eastern sector</lookup>\n",
    "\n",
    "Observation 2\n",
    "The eastern sector extends into the High Plains and is called the Central Plains orogeny.\n",
    "\n",
    "Thought 3\n",
    "The eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n",
    "\n",
    "Action 3\n",
    "<search>High Plains</search>\n",
    "\n",
    "Observation 3\n",
    "High Plains refers to one of two distinct land regions\n",
    "\n",
    "Thought 4\n",
    "I need to instead search High Plains (United States).\n",
    "\n",
    "Action 4\n",
    "<search>High Plains (United States)</search>\n",
    "\n",
    "Observation 4\n",
    "The High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130m).\n",
    "\n",
    "Thought 5\n",
    "High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n",
    "\n",
    "Action 5\n",
    "<finish>1,800 to 7,000 ft</finish>\n",
    "\"\"\"\n",
    "\n",
    "# Come up with more examples yourself, or take a look through https://github.com/ysymyth/ReAct/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cb414369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought 1\n",
      "I need to search for \"transformers NLP paper\" to find the paper and then identify the authors and find the youngest author.\n",
      "\n",
      "Action 1\n",
      "<search>transformers NLP paper</search>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"Question\n",
    "Who was the youngest author listed on the transformers NLP paper?\n",
    "\"\"\"\n",
    "\n",
    "# You will perform the Action; so generate up to, but not including, the Observation.\n",
    "react_config = types.GenerateContentConfig(\n",
    "    stop_sequences=[\"\\nObservation\"],\n",
    "    system_instruction=model_instructions + example1 + example2,\n",
    ")\n",
    "\n",
    "# Create a chat that has the model instructions and examples pre-seeded.\n",
    "react_chat = client.chats.create(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=react_config,\n",
    ")\n",
    "\n",
    "resp = react_chat.send_message(question)\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "30c96f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Thought 3\n",
       "Searching for Ashish Vaswani only returned the paper itself. I will try searching for another author.\n",
       "\n",
       "Action 3\n",
       "<search>Aidan N. Gomez</search>\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation = \"\"\"Observation 1\n",
    "[1706.03762] Attention Is All You Need\n",
    "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\n",
    "We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n",
    "\"\"\"\n",
    "resp = react_chat.send_message(observation)\n",
    "Markdown(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b4ede2",
   "metadata": {},
   "source": [
    "Thinking Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b0598746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The youngest author listed on the \"Attention is All You Need\" paper, which introduced the Transformer architecture and is highly influential in NLP, is likely **Aidan N. Gomez**.\n",
       "\n",
       "Here's why and how we can deduce this:\n",
       "\n",
       "* **\"Attention is All You Need\" Paper:** This is the foundational paper for Transformers in NLP.  You can easily find it by searching for this title or \"Transformer paper NLP\".\n",
       "* **Authors of the Paper:** The authors are: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin.\n",
       "* **Inferring Age (Indirectly):**  We don't have birthdates explicitly listed in the paper. However, we can infer relative career stages based on typical academic paths and online information:\n",
       "    * **Aidan N. Gomez:**  At the time of the paper's publication (2017), Aidan N. Gomez was a **PhD student at the University of Toronto**, working with Geoffrey Hinton.  PhD students are generally among the youngest researchers in a collaborative effort like this.\n",
       "    * **Other Authors:** The other authors were primarily affiliated with **Google Brain**.  Researchers at Google Brain are typically more established and further along in their careers than PhD students.  While some might be relatively early career, a PhD student is generally going to be younger.\n",
       "\n",
       "**Therefore, based on typical academic career stages and publicly available information, Aidan N. Gomez, being a PhD student at the time, is highly likely to be the youngest author listed on the \"Attention is All You Need\" paper.**\n",
       "\n",
       "It's important to note that we are inferring \"youngest\" based on career stage, not absolute birthdates, as that information isn't readily available or relevant to the paper itself."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "from IPython.display import Markdown, clear_output # type: ignore\n",
    "\n",
    "\n",
    "response = client.models.generate_content_stream(\n",
    "    model='gemini-2.0-flash-thinking-exp',\n",
    "    contents='Who was the youngest author listed on the transformers NLP paper?',\n",
    ")\n",
    "\n",
    "buf = io.StringIO()\n",
    "for chunk in response:\n",
    "    buf.write(chunk.text)\n",
    "    # Display the response as it is streamed\n",
    "    print(chunk.text, end='')\n",
    "\n",
    "# And then render the finished response as formatted markdown.\n",
    "clear_output()\n",
    "Markdown(buf.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ad27d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
